#-------------------------------------------------------------------------
# AUTHOR: Kayla Nguyen
# FILENAME: kaylanguyen_assignment3q3.py
# SPECIFICATION: Building a base classifier by using a single decision
# tree, an ensemble classifier, and a Random Forest classifier
# FOR: CS 4210- Assignment #3
# TIME SPENT: 4 hours
#-----------------------------------------------------------*/

#IMPORTANT NOTE: DO NOT USE ANY ADVANCED PYTHON LIBRARY TO COMPLETE THIS CODE SUCH AS numpy OR pandas. You have to work here only with standard vectors and arrays

#importing some Python libraries
from sklearn import tree
from sklearn.utils import resample
from sklearn.ensemble import RandomForestClassifier
import csv
dbTraining = []
dbTest = []
X_training = []
y_training = []
classVotes = [] #this array will be used to count the votes of each classifier

#reading the training data from a csv file and populate dbTraining
with open('optdigits.tra', 'r') as csvfile:
  reader = csv.reader(csvfile)
  for i, row in enumerate(reader):
      if i > 0: #skipping the header
         dbTraining.append (row)

#reading the test data from a csv file and populate dbTest
with open('optdigits.tes', 'r') as csvfile:
  reader = csv.reader(csvfile)
  for i, row in enumerate(reader):
      if i > 0: #skipping the header
         dbTest.append (row)

#inititalizing the class votes for each test sample. Example: classVotes.append([0,0,0,0,0,0,0,0,0,0])
#--> add your Python code here
classVotes= [[0,0,0,0,0,0,0,0,0,0] for row in range(len(dbTest))]

print("Started my base and ensemble classifier ...")

correct1 = 0
incorrect1 = 0
correct2 = 0
incorrect2 = 0
correct3 = 0
incorrect3 = 0
for k in range(20): #we will create 20 bootstrap samples here (k = 20). One classifier will be created for each bootstrap sample
    bootstrapSample = resample(dbTraining, n_samples=len(dbTraining), replace=True)
    #populate the values of X_training and y_training by using the bootstrapSample
    for sample in bootstrapSample:
        X_training.append(sample[:-1])
        y_training.append(sample[-1])

  #fitting the decision tree to the data
    clf = tree.DecisionTreeClassifier(criterion = 'entropy', max_depth=None) #we will use a single decision tree without pruning it
    clf = clf.fit(X_training, y_training)

    for i, testSample in enumerate(dbTest):
      #make the classifier prediction for each test sample and update the corresponding index value in classVotes. For instance,
      # if your first base classifier predicted 2 for the first test sample, then classVotes[0,0,0,0,0,0,0,0,0,0] will change to classVotes[0,0,1,0,0,0,0,0,0,0].
      # Later, if your second base classifier predicted 3 for the first test sample, then classVotes[0,0,1,0,0,0,0,0,0,0] will change to classVotes[0,0,1,1,0,0,0,0,0,0]
      # Later, if your third base classifier predicted 3 for the first test sample, then classVotes[0,0,1,1,0,0,0,0,0,0] will change to classVotes[0,0,1,2,0,0,0,0,0,0]
      # this array will consolidate the votes of all classifier for all test samples
        y_predict = int(clf.predict([testSample[:-1]])[0])
        classVotes[i][y_predict] = classVotes[i][y_predict] + 1
        y_test = int(testSample[-1])
        if k == 0: #for only the first base classifier, compare the prediction with the true label of the test sample here to start calculating its accuracy
            if y_test == y_predict:
                correct1 = correct1 + 1
            else:
                incorrect1 = incorrect1 + 1

    if k == 0: #for only the first base classifier, print its accuracy here
        accuracy = correct1 / (incorrect1 + correct1)
        
        print("Finished my base classifier (fast but relatively low accuracy) ...")
        print("My base classifier accuracy: " + str(accuracy))
        print("")



  #now, compare the final ensemble prediction (majority vote in classVotes) for each test sample with the ground truth label to calculate the accuracy of the ensemble classifier (all base classifiers together)
  #--> add your Python code here
    for i, testSample in enumerate(dbTest):
        majority_vote = int(classVotes[i].index(max(classVotes[i])))
        y_test = int(testSample[-1])
        if majority_vote == y_test:
            correct2 = correct2 + 1
        else:
            incorrect2 = incorrect2 + 1


  #printing the ensemble accuracy here
    accuracy = correct2 / (correct2 + incorrect2)
    print("Finished my ensemble classifier (slow but higher accuracy) ...")
    print("My ensemble accuracy: " + str(accuracy))
    print("")

    print("Started Random Forest algorithm ...")


  #Create a Random Forest Classifier
    clf=RandomForestClassifier(n_estimators=20) #this is the number of decision trees that will be generated by Random Forest. The sample of the ensemble method used before

  #Fit Random Forest to the training data
    clf.fit(X_training,y_training)

  #make the Random Forest prediction for each test sample. Example: class_predicted_rf = clf.predict([[3, 1, 2, 1, ...]]
  #--> add your Python code here
    for i, testSample in enumerate(dbTest):
        class_predicted_rf = int(clf.predict([testSample[:-1]]))
        y_test = int(testSample[-1])
  #compare the Random Forest prediction for each test sample with the ground truth label to calculate its accuracy
  #--> add your Python code here
        if class_predicted_rf == y_test:
            correct3 = correct3 + 1
        else:
            incorrect3 = incorrect3 + 1
        accuracy = correct3 / (correct3+incorrect3)
  #printing Random Forest accuracy here
    print("Random Forest accuracy: " + str(accuracy))
    print("")

print("Finished Random Forest algorithm (much faster and higher accuracy!) ...")